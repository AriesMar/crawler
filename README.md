# Python é€šç”¨çˆ¬è™«æ¡†æ¶

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€æ˜“äºæ‰©å±•çš„Pythonçˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ–¹å¼å’Œåçˆ¬è™«ç­–ç•¥ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- **é€šç”¨æ¡†æ¶è®¾è®¡**ï¼šåŸºäºæŠ½è±¡åŸºç±»ï¼Œæ˜“äºæ‰©å±•æ–°çš„çˆ¬è™«ç±»å‹
- **æ™ºèƒ½é‡è¯•æœºåˆ¶**ï¼šè‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿
- **åçˆ¬è™«ç­–ç•¥**ï¼šéšæœºUser-Agentã€è¯·æ±‚å»¶è¿Ÿã€ä»£ç†æ”¯æŒ
- **å¤šç§å­˜å‚¨æ–¹å¼**ï¼šæ”¯æŒJSONã€CSVã€SQLiteã€MongoDB
- **è¯¦ç»†æ—¥å¿—è®°å½•**ï¼šå½©è‰²æ—¥å¿—è¾“å‡ºï¼Œæ”¯æŒæ–‡ä»¶è½®è½¬
- **Seleniumæ”¯æŒ**ï¼šå¤„ç†åŠ¨æ€ç½‘é¡µå†…å®¹
- **é…ç½®åŒ–ç®¡ç†**ï¼šç¯å¢ƒå˜é‡é…ç½®ï¼Œçµæ´»çš„å‚æ•°è°ƒæ•´
- **æ’ä»¶ç³»ç»Ÿ**ï¼šæ”¯æŒè‡ªå®šä¹‰æ’ä»¶æ‰©å±•åŠŸèƒ½
- **å·¥å…·å‡½æ•°åº“**ï¼šä¸°å¯Œçš„è¾…åŠ©å·¥å…·å’ŒéªŒè¯å‡½æ•°

## ğŸ“ é¡¹ç›®ç»“æ„

```
crawler/
â”œâ”€â”€ crawler_framework/          # ä¸»æ¡†æ¶åŒ…
â”‚   â”œâ”€â”€ __init__.py            # æ¡†æ¶å…¥å£
â”‚   â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ core/                  # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_crawler.py    # åŸºç¡€çˆ¬è™«ç±»
â”‚   â”‚   â”œâ”€â”€ request_manager.py # è¯·æ±‚ç®¡ç†å™¨
â”‚   â”‚   â””â”€â”€ data_storage.py    # æ•°æ®å­˜å‚¨ç®¡ç†å™¨
â”‚   â”œâ”€â”€ utils/                 # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py          # æ—¥å¿—å·¥å…·
â”‚   â”‚   â”œâ”€â”€ helpers.py         # è¾…åŠ©å·¥å…·å‡½æ•°
â”‚   â”‚   â””â”€â”€ validators.py      # éªŒè¯å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ plugins/               # æ’ä»¶ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_plugin.py     # æ’ä»¶åŸºç±»
â”‚   â”‚   â””â”€â”€ plugin_manager.py  # æ’ä»¶ç®¡ç†å™¨
â”‚   â””â”€â”€ crawlers/              # çˆ¬è™«å®ç°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ wechat_crawler.py  # å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«
â”‚       â””â”€â”€ news_crawler.py    # æ–°é—»ç½‘ç«™çˆ¬è™«
â”œâ”€â”€ examples/                  # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ basic_usage.py        # åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ tests/                    # æµ‹è¯•æ–‡ä»¶
â”‚   â””â”€â”€ test_framework.py     # æ¡†æ¶æµ‹è¯•
â”œâ”€â”€ requirements.txt          # é¡¹ç›®ä¾èµ–
â”œâ”€â”€ env_example.txt          # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ logs/                    # æ—¥å¿—æ–‡ä»¶ç›®å½•
â”œâ”€â”€ data/                    # æ•°æ®å­˜å‚¨ç›®å½•
â””â”€â”€ downloads/               # ä¸‹è½½æ–‡ä»¶ç›®å½•
```

## ğŸ› ï¸ å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

## âš¡ å¿«é€Ÿå¼€å§‹

### 1. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `env_example.txt` ä¸º `.env` å¹¶ä¿®æ”¹é…ç½®ï¼š

```bash
cp env_example.txt .env
```

### 2. è¿è¡Œæµ‹è¯•

```bash
python tests/test_framework.py
```

### 3. è¿è¡Œç¤ºä¾‹

```bash
python examples/basic_usage.py
```

### 4. ä½¿ç”¨å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«

```python
from crawler_framework import WeChatCrawler

# å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URLåˆ—è¡¨
urls = [
    "https://mp.weixin.qq.com/s/your_article_url_1",
    "https://mp.weixin.qq.com/s/your_article_url_2",
]

# ä½¿ç”¨çˆ¬è™«
with WeChatCrawler(storage_type='json') as crawler:
    results = crawler.crawl_wechat_articles(urls)
    
    for article in results:
        print(f"æ ‡é¢˜: {article['title']}")
        print(f"ä½œè€…: {article['author']}")
        print(f"å†…å®¹: {article['content'][:100]}...")
```

## ğŸ”§ åˆ›å»ºè‡ªå®šä¹‰çˆ¬è™«

ç»§æ‰¿ `BaseCrawler` ç±»åˆ›å»ºæ–°çš„çˆ¬è™«ï¼š

```python
from crawler_framework import BaseCrawler
from bs4 import BeautifulSoup

class MyCrawler(BaseCrawler):
    def get_urls(self):
        """è¿”å›è¦çˆ¬å–çš„URLåˆ—è¡¨"""
        return ["https://example.com/page1", "https://example.com/page2"]
    
    def parse(self, response):
        """è§£æå“åº”æ•°æ®"""
        soup = BeautifulSoup(response.text, 'html.parser')
        
        return {
            'title': soup.find('h1').text.strip(),
            'content': soup.find('div', class_='content').text.strip(),
            'url': response.url
        }

# ä½¿ç”¨è‡ªå®šä¹‰çˆ¬è™«
with MyCrawler(storage_type='csv') as crawler:
    results = crawler.crawl()
```

## âš™ï¸ é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®
- `DEBUG`: è°ƒè¯•æ¨¡å¼å¼€å…³
- `LOG_LEVEL`: æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)

### è¯·æ±‚é…ç½®
- `REQUEST_TIMEOUT`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `REQUEST_DELAY`: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
- `MAX_RETRIES`: æœ€å¤§é‡è¯•æ¬¡æ•°

### å­˜å‚¨é…ç½®
- `DATABASE_URL`: SQLiteæ•°æ®åº“URL
- `MONGO_URI`: MongoDBè¿æ¥URI
- `MONGO_DB`: MongoDBæ•°æ®åº“å

### ä»£ç†é…ç½®
- `USE_PROXY`: æ˜¯å¦ä½¿ç”¨ä»£ç†
- `PROXY_LIST`: ä»£ç†æœåŠ¡å™¨åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰

## ğŸ’¾ æ•°æ®å­˜å‚¨

æ¡†æ¶æ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ–¹å¼ï¼š

### JSONå­˜å‚¨
```python
crawler = WeChatCrawler(storage_type='json')
```

### CSVå­˜å‚¨
```python
crawler = WeChatCrawler(storage_type='csv')
```

### SQLiteå­˜å‚¨
```python
crawler = WeChatCrawler(storage_type='sqlite')
```

### MongoDBå­˜å‚¨
```python
crawler = WeChatCrawler(storage_type='mongodb')
```

## ğŸ”Œ æ’ä»¶ç³»ç»Ÿ

### åˆ›å»ºè‡ªå®šä¹‰æ’ä»¶

```python
from crawler_framework.plugins import BasePlugin

class MyPlugin(BasePlugin):
    def __init__(self):
        super().__init__("MyPlugin", "1.0.0")
        self.description = "æˆ‘çš„è‡ªå®šä¹‰æ’ä»¶"
    
    def initialize(self, config):
        """åˆå§‹åŒ–æ’ä»¶"""
        self.config = config
        return True
    
    def execute(self, data):
        """æ‰§è¡Œæ’ä»¶é€»è¾‘"""
        # å¤„ç†æ•°æ®
        data['processed'] = True
        return data

# ä½¿ç”¨æ’ä»¶
from crawler_framework.plugins import PluginManager

plugin_manager = PluginManager()
plugin = MyPlugin()
plugin_manager.register_plugin(plugin)

# æ‰§è¡Œæ’ä»¶
result = plugin_manager.execute_plugin("MyPlugin", data)
```

## ğŸ› ï¸ å·¥å…·å‡½æ•°

æ¡†æ¶æä¾›äº†ä¸°å¯Œçš„å·¥å…·å‡½æ•°ï¼š

```python
from crawler_framework.utils.helpers import clean_text, extract_domain, generate_filename
from crawler_framework.utils.validators import validate_url, validate_email

# æ–‡æœ¬æ¸…ç†
clean_text("  è¿™æ˜¯  ä¸€ä¸ª  æµ‹è¯•  æ–‡æœ¬  ")

# åŸŸåæå–
extract_domain("https://www.example.com/path")

# æ–‡ä»¶åç”Ÿæˆ
generate_filename("crawler", "json")

# URLéªŒè¯
validate_url("https://www.example.com")
```

## ğŸ“Š å¾®ä¿¡å…¬ä¼—å·çˆ¬è™«ç‰¹æ€§

- **å®Œæ•´å†…å®¹æå–**ï¼šæ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¶é—´ã€æ­£æ–‡ã€å›¾ç‰‡ç­‰
- **å›¾ç‰‡URLæå–**ï¼šè‡ªåŠ¨æå–æ–‡ç« ä¸­çš„æ‰€æœ‰å›¾ç‰‡é“¾æ¥
- **æ ‡ç­¾ç”Ÿæˆ**ï¼šåŸºäºå†…å®¹å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾
- **å…ƒæ•°æ®è·å–**ï¼šå…¬ä¼—å·åç§°ã€IDç­‰å…ƒä¿¡æ¯
- **åæ£€æµ‹**ï¼šæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º

## ğŸ“° æ–°é—»çˆ¬è™«ç‰¹æ€§

- **é€šç”¨è§£æ**ï¼šæ”¯æŒå¤šç§æ–°é—»ç½‘ç«™ç»“æ„
- **æ™ºèƒ½æå–**ï¼šè‡ªåŠ¨è¯†åˆ«æ ‡é¢˜ã€å†…å®¹ã€ä½œè€…ç­‰ä¿¡æ¯
- **åˆ†ç±»æ ‡ç­¾**ï¼šæå–æ–°é—»åˆ†ç±»å’Œæ ‡ç­¾
- **å›¾ç‰‡å¤„ç†**ï¼šè‡ªåŠ¨å¤„ç†ç›¸å¯¹å’Œç»å¯¹å›¾ç‰‡URL

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éµå®ˆrobots.txt**ï¼šè¯·ç¡®ä¿éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtè§„åˆ™
2. **åˆç†ä½¿ç”¨**ï¼šæ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›
3. **æ³•å¾‹åˆè§„**ï¼šç¡®ä¿çˆ¬å–è¡Œä¸ºç¬¦åˆç›¸å…³æ³•å¾‹æ³•è§„
4. **æ•°æ®ä½¿ç”¨**ï¼šä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ï¼Œä¸å¾—ç”¨äºå•†ä¸šç”¨é€”

## ğŸ”§ æ‰©å±•åŠŸèƒ½

### æ·»åŠ æ–°çš„å­˜å‚¨æ–¹å¼
åœ¨ `crawler_framework/core/data_storage.py` ä¸­æ·»åŠ æ–°çš„å­˜å‚¨æ–¹æ³•ã€‚

### æ·»åŠ æ–°çš„çˆ¬è™«ç±»å‹
ç»§æ‰¿ `BaseCrawler` ç±»å¹¶å®ç° `parse()` å’Œ `get_urls()` æ–¹æ³•ã€‚

### è‡ªå®šä¹‰è¯·æ±‚ç­–ç•¥
ä¿®æ”¹ `crawler_framework/core/request_manager.py` ä¸­çš„è¯·æ±‚é€»è¾‘ã€‚

### åˆ›å»ºè‡ªå®šä¹‰æ’ä»¶
ç»§æ‰¿ `BasePlugin` ç±»å¹¶å®ç° `initialize()` å’Œ `execute()` æ–¹æ³•ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªæ¡†æ¶ï¼

## ï¿½ï¿½ è®¸å¯è¯

MIT License
